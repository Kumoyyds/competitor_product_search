{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0769fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from llm_tools.llm_func import do_product_searching\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b35b5",
   "metadata": {},
   "source": [
    "# Loading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45483427",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f320afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input path\n",
    "input_file_name = \"amazon_de_url.xlsx\"\n",
    "input_path = os.path.join(os.getcwd(), f\"input/{input_file_name}\")\n",
    "file = pd.read_excel(input_path)\n",
    "\n",
    "# output path \n",
    "output_path = 'result.xlsx'\n",
    "output_path = os.path.join(os.getcwd(), output_path)\n",
    "\n",
    "# output_partition path\n",
    "output_p_root = os.path.join(os.getcwd(), \"/output_partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f9890",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_match = 0\n",
    "\n",
    "\n",
    "match_file_path = 'match_file'\n",
    "\n",
    "if use_match:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    match_file_path = os.path.join(os.getcwd(), match_file_path)\n",
    "    match_file = pd.read_excel(match_file_path)\n",
    "    # make sure the category and web col name unified as well \n",
    "    match_file['Category'] = match_file['Category'].apply(lambda x: x.strip())\n",
    "    # the cate used to do match should be defined as well \n",
    "\n",
    "    match_dict = {}\n",
    "    for i in range(match_file.shape[0]):\n",
    "        match_dict[match_file.iloc[i, 0]] = list(match_file.iloc[i, 1:])\n",
    "\n",
    "else:\n",
    "    # be in the yaml\n",
    "    web = web\n",
    "    \n",
    "    file['web_1'] = web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56caab",
   "metadata": {},
   "source": [
    "# matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be639a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_match:\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    def embedding_one(text):\n",
    "        return model.encode(text)\n",
    "# the categories associated with files\n",
    "    file_embedding = {}\n",
    "    for i in tqdm(set(file['suggest_cate_1']), total=len(set(file['suggest_cate_1']))):\n",
    "        file_embedding[i] = embedding_one(i)\n",
    "\n",
    "# the categories associated with the web\n",
    "    cate_embedding = {}\n",
    "    for i in tqdm(match_dict, total=len(match_dict)):\n",
    "        cate_embedding[i] = embedding_one(i)\n",
    "    cate_list = [i for i in match_dict]\n",
    "    cate_matrix = np.vstack([cate_embedding[i] for i in cate_list])\n",
    "\n",
    "    file.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    cate_col = 'suggest_cate_1'\n",
    "    n_web = len(match_dict[list(match_dict.keys())[0]])\n",
    "\n",
    "    results = [[] for i in range(n_web)]\n",
    "    match_c = []\n",
    "    sim_v = []\n",
    "    for i in range(file.shape[0]):\n",
    "        v = file_embedding[file.loc[i, cate_col]]\n",
    "    # the similarity\n",
    "    sim = cate_matrix @ v\n",
    "\n",
    "    # max similarity\n",
    "    sim_max_i = np.argmax(sim)\n",
    "\n",
    "    # matched_cate\n",
    "    c = cate_list[sim_max_i]\n",
    "\n",
    "    sim_v.append(sim[sim_max_i])\n",
    "\n",
    "    match_c.append(c)\n",
    "    item = match_dict[c]\n",
    "    for j in range(n_web):\n",
    "        results[j].append(item[j])\n",
    "\n",
    "    file['match_c'] = match_c\n",
    "    file['sim'] = sim_v\n",
    "    for i in range(n_web):\n",
    "        file[f\"web_{i+1}\"] = results[i]   \n",
    "\n",
    "\n",
    "    ### setting in yaml\n",
    "    sim_cri = sim_cri\n",
    "    mask = (file['sim']>sim_cri) \n",
    "    # done\n",
    "    file = file.loc[mask]\n",
    "    file.reset_index(drop=True, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### yaml definition\n",
    "def find_the_url(df, name_col=name_col, web_col='web_1', country=country, url_col_name=url_col_name):\n",
    "#def find_the_url(df, name_col='item_sku_name_en', web_col='web_1', country='uk', url_col_name='url_search_1'):\n",
    "  result = []\n",
    "  df.reset_index(drop=True, inplace=True)\n",
    "  for i in range(df.shape[0]):\n",
    "    product_name = df.loc[i, name_col]\n",
    "    mkt_plc = df.loc[i, web_col]\n",
    "    r = do_product_searching(product_name, mkt_plc, country=country)\n",
    "    result.append(r)\n",
    "  df[url_col_name] = result\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109aea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_num(num):\n",
    "  i = 1\n",
    "  while num // 10 > 0:\n",
    "    num = num // 10\n",
    "    i *= 10\n",
    "  return num * i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecf5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_num = get_split_num(file.shape[0])\n",
    "files = np.array_split(file, split_num)\n",
    "\n",
    "\n",
    "round = split_num//1000 + 1\n",
    "chunk = int(split_num/round)\n",
    "for i in range(round):\n",
    "  print(f\"round {i+1} begins\")\n",
    "  dfs = files[i*chunk:(i+1)*chunk]\n",
    "  with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    dfs = list(tqdm(executor.map(find_the_url, dfs), total=chunk))\n",
    "  df = pd.concat(dfs, axis=0)\n",
    "  df.reset_index(drop=True, inplace=True)\n",
    "  # after 30000, use 2\n",
    "  df.to_excel(root+f'result_{i}.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
